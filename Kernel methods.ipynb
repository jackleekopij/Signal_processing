{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Kernel methods owe their name from the use of kernel functions used to measure similarity between two vectors.\n",
    "\n",
    "Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This implicit calculation of the data into the higher dimensional space without actually calculating the projection is referred to as the **kernel trick**. \n",
    "\n",
    "Further kernel functions provide a method to measure the similarity between vectors (data points). For example the value for the a kernel function $\\space \\kappa(\\textbf{x}_i, \\textbf{x}_j) $ is relatively high when data is similar and relatively low when dissimilar.\n",
    "\n",
    "### Instance based learners\n",
    "Kernel methods are included in a subset of machine learning algorithms termed *instance based learners*. Opposed to other *parameteric methods* where a set of parameters are learnt to map inputs to some output space,  $\\space f_\\theta(x): x \\rightarrow y $ . Instance based learning instead learns weights $ w_i $ for each training example $ \\space (\\textbf{x}_i, y_i) $ recalling these weights at test/prediction time to quantify similarity between training inputs $\\textbf{x}_i$ and test/prediction point $\\textbf{x'}$ predicting output for  $ \\space \\textbf{x'}$. Predictions for $ \\hat y$ are calculated by use of the four inputs, \n",
    "\n",
    "1. Training input data (indexed by i) \n",
    "2. Training outputs (indexed by i) \n",
    "3. Training weights (indexed by i) \n",
    "4. Test/prediciton vector \n",
    "\n",
    "A binary classifier is a concrete example of an *instance based learner* which can be calculaed using the following: \n",
    "\n",
    "$$ \\hat y = sgn \\sum^n_{i=1}y_i w_i k(\\textbf{x}_i, x') $$\n",
    "\n",
    "Where parameters are defined by: \n",
    "\n",
    "$ k(\\textbf{x}_i, x') $ is the kernel similarity that measures the similarity between training data point i and test/prediction point. \n",
    "\n",
    "$ y_i $ is a class label. Either {0,1}. \n",
    "\n",
    "$ w_i $ is a learnt weight vector. \n",
    "\n",
    "$ sgn $ a function to determine class ie if > 0 positive class else negative class.\n",
    "\n",
    "$ \\hat y $ is the predicted class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squared kernel \n",
    "\n",
    "$$ \\space \\kappa(\\textbf{x}_i, \\textbf{x}_j) =  <\\phi(x_i), \\phi(x_j)> $$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ \\phi(x_i) = \\textbf{x}_i^2 $$\n",
    "\n",
    "[x_1 x_2] ([x_1 x_2])T = (x_1^2 +  x_2^2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4761\n"
     ]
    }
   ],
   "source": [
    "# Create data \n",
    "b = np.array([3,5])\n",
    "a = np.array([8,9])\n",
    "\n",
    "print(np.dot(a,b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
