{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No image generated.\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import os \n",
    "%load_ext tikzmagic\n",
    "%tikz \\draw (0,0) rectangle (1,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Model: parameters\n",
    "A Markov process with unobservable (hidden) states is referred to as as a Hidden Markov Model (HMM). Explicitly, such a process evolves through time or space with state dependency defined by the Markov property, $P(Z_{t+1} | Z_t) \\space = \\space P(Z_{t+1} | Z_t, ... Z_0)$. The variable here, $Z_k$ is interpretted as the hidden state of the sequence at time k. As this hidden process evolves the hidden state, $Z_k$ generates observable variables termed $X_k$, defined as observation of the process at time k. It is assumed that the observed variables are only dependent upon the current state and not influenced by previous observable states.\n",
    "\n",
    "Distilling the above concepts, a Hidden Markov Model is completely defined by three components: \n",
    "<br>\n",
    "    &emsp; **Initial distribution**, a probability distribution across all initial/starting states: \n",
    "        $ \\pi_i $\n",
    "<br>\n",
    "    &emsp; A **(hidden) state transition matrix**, models how the process evolves from one hidden state to the next. Again, the Markov property is assumed whereby the hidden state at time t+1, $Z_{t+1}$, is only dependent on the hidden state at time t, $Z_t$: \n",
    "        $ P(Z_{t+1} = j |Z_{t} = i) $ \n",
    "<br>\n",
    "    &emsp; Finally, the **emission matrix**, is a distribution which defines a probability distribution of the observation $X_t$ given the hidden state $Z_t$.\n",
    "        $ P(X_t = k |Z_t = j) $ \n",
    " \n",
    " \n",
    "Once a HMM is learned, the aim is to perform inference of hidden states of the evolved hidden probabilities. Being able to calaculate the different aspects (online & offline belief state prediction and most probable sequence covered later) of this hidden sequence is provide a powerful predictive toolset. Due to this predictive power and HMMs natural ability to model sequential dependencies; HMMs have applications in speech & handwriting recognition, genetics, natural language processing and reinforcement learning. Each of application is really concerned with the inverse of the distributions defined above. At inference time the exact hidden state (also termed the belief state) is the objective, $P(Z_t = j|X_t=i)$. Proof of derivation for this distribution is provided below though it is a simply an application of incorporating all three components: *initial distribution*, *state transition matrix* and *emission materix*.   \n",
    "\n",
    " \n",
    " \n",
    "### Major inference methods\n",
    "HMM inference has numerous applications and derivations; at the heart of HMM inferential tasks inferring the hidden state sequence, given estimates both observations of data and estimates for the above parameters.\n",
    "\n",
    "#### Online vs offline\n",
    "An important distinction for HMM inference methods is the difference between an _online_ vs _offline_ model. _Online_ methods can be used while simultaneously collecting data, useful for predicting a current hidden state (_filtering_, covered below). _Offline_ inference requires the sequence be _complete_, i.e. all the data is collected up to time T, the end of the sequence. A the cost of losing real-time prediction, _offline_ algorithms provide the added benefit of applying hindsight to predictions earlier in the chain decreasing the overall uncertainty.\n",
    "\n",
    "__Filtering__; a method with the aim to derive the current _belief state_ of the _hidden state_ given the current and past _observed data_. Explicitly, the distribution for filtering is written as <br> $$ P(Z_t |X_{1:t})$$ \n",
    "\n",
    "\n",
    "__Smoothing__; a method to compute the belief of hidden states, _retrospectively_. _Smoothing_ calculates the distribuiton \n",
    "\n",
    "<br> $$ P(Z_t | X_{1:T}) $$\n",
    "\n",
    "where T is the final time step of the sequence. By using the entire sequence of data to _retospectively_ update beliefs of hidden states; overall uncertainty is reduced i.e. given the current hidden states affect the future observations, observing future observations provides evidence for current hidden states. \n",
    "\n",
    "### Algorithms\n",
    "\n",
    "<br>\n",
    "    1. Forward: filtering\n",
    "        The forward algorithm is an **online** algortihm used to compute the belief state of the latent (hidden) variable at the present time. b\n",
    "    2. Forward-backward: smoothing\n",
    "    3. Viterbi: maximum path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in emission and transition probability matrices\n",
    "HMM_PARAMS = pd.read_json(\"hmm_forward_alg_params.json\")\n",
    "HMM_EMISSION = HMM_PARAMS['emissions']\n",
    "HMM_INITIAL = HMM_PARAMS['initial']\n",
    "HMM_TRANSITION = HMM_PARAMS['transition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fair', 'loaded'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FORWARD ALGORITHM\n",
    "The forward algorithm is useful for deriving the current _belief state, $Z_t$,_ given observations up to time _t_. \n",
    "\n",
    "Define the game of loaded casino: \n",
    "    '664153216162115234653214356634261655234232315142464156663246'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROLL_SEQUENCE = '664153216162115234653214356634261655234232315142464156663246'\n",
    "observed_sequence = ROLL_SEQUENCE\n",
    "initial_observation = observed_sequence[0]\n",
    "\n",
    "initial_dist = HMM_INITIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.074999999999999997, 0.25]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_observation = observed_sequence[0]\n",
    "alpha_1 = [ HMM_EMISSION[i][initial_observation] * HMM_INITIAL[i] for i in initial_dist.keys()]\n",
    "alpha_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_dist(observed_sequence, initial_dist, transition, emission): \n",
    "    initial_observation = observed_sequence[0]\n",
    "    hidden_states = initial_dist.keys()\n",
    "\n",
    "    alpha_1_to_t = []\n",
    "    \n",
    "    # Calculate alpha for initial observation\n",
    "    alpha_1 = [HMM_EMISSION[i][initial_observation] * HMM_INITIAL[i] for i in hidden_states ]\n",
    "    alpha_1_normalised = np.array( alpha_1 / sum(alpha_1) )\n",
    "    \n",
    "    alpha_1_to_t.append(alpha_1_normalised)\n",
    "    alpha_t_normalised = alpha_1_normalised\n",
    "    \n",
    "    \n",
    "    # Iterate belief state calculation through remaining sequence\n",
    "    for observation in observed_sequence[1:]:\n",
    "        alpha_t = [ ]\n",
    "        for j in hidden_states:\n",
    "            temp = 0\n",
    "            for index,i in enumerate(hidden_states):\n",
    "                temp += HMM_EMISSION[j][observation] * transition[i][j] * alpha_t_normalised[index]\n",
    "            alpha_t.append(temp)\n",
    "            \n",
    "        alpha_t_normalised = normalise_array(alpha_t)\n",
    "        \n",
    "        # Append result to 'belief state' list for time t\n",
    "        alpha_1_to_t.append(alpha_t_normalised)\n",
    "    \n",
    "    #return 'belief state' list \n",
    "    return alpha_1_to_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalise_array(array): \n",
    "    if isinstance(array,list):\n",
    "        return np.array(array/sum(array))\n",
    "    \n",
    "    if isinstance(array,numpy.ndarray):\n",
    "        return array/sum(array)\n",
    "    \n",
    "    \n",
    "    \n",
    "def binary_softmax(tuple_sequence, labels = [0,1], threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Return list of transformed sequence as bi-labelled data (default to binary) determined by threshold. \n",
    "    \"\"\"\n",
    "    binary_list = [ ]\n",
    "    \n",
    "    for i in tuple_sequence: \n",
    "        if i[0] > threshold:\n",
    "            binary_list.append(labels[0])\n",
    "        else: \n",
    "            binary_list.append(labels[1])\n",
    "    \n",
    "    return binary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "belief_state_seq = forward_dist(ROLL_SEQUENCE, HMM_INITIAL, HMM_TRANSITION, HMM_EMISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loaded', 'loaded', 'loaded', 'loaded', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'loaded', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'loaded', 'fair', 'fair', 'fair', 'fair', 'fair', 'loaded', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'fair', 'loaded', 'loaded', 'loaded', 'fair', 'fair', 'loaded']\n",
      "664153216162115234653214356634261655234232315142464156663246\n"
     ]
    }
   ],
   "source": [
    "print(binary_softmax(belief_state_seq, labels = [\"fair\",\"loaded\"]))\n",
    "print(ROLL_SEQUENCE)\n",
    "for i in belief_state_seq: \n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
